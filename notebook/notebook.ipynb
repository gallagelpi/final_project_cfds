{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'best_library'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ====================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# End-to-End Pipeline Notebook\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ====================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# --- 0️⃣ Imports ---\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbest_library\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     LoadData, Preprocessing, FeatureBuilder, DatasetSplitter,\n\u001b[32m      9\u001b[39m     ModelBuilder, Trainer, Predictor, HyperparameterTuner, Evaluator\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# --- 1️⃣ Configuration ---\u001b[39;00m\n\u001b[32m     13\u001b[39m DATASET_DIR = \u001b[33m\"\u001b[39m\u001b[33m../dataset\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# raw dataset\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'best_library'"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# End-to-End Pipeline Notebook\n",
    "# ====================================================\n",
    "\n",
    "# --- 0️⃣ Imports ---\n",
    "import torch\n",
    "from best_library import (\n",
    "    LoadData, Preprocessing, FeatureBuilder, DatasetSplitter,\n",
    "    ModelBuilder, Trainer, Predictor, HyperparameterTuner, Evaluator\n",
    ")\n",
    "\n",
    "# --- 1️⃣ Configuration ---\n",
    "DATASET_DIR = \"../dataset\"   # raw dataset\n",
    "WORK_DIR = \"../data\"         # working directory for split dataset\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "IMG_SIZE = 224\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_PATH = \"../models/best_model.pth\"\n",
    "\n",
    "# ====================================================\n",
    "# a) Preprocessing\n",
    "# ====================================================\n",
    "preprocessor = Preprocessing(img_size=IMG_SIZE)\n",
    "transform = preprocessor.get_transform()\n",
    "\n",
    "# ====================================================\n",
    "# b) Feature building\n",
    "# ====================================================\n",
    "feature_builder = FeatureBuilder()\n",
    "# Example usage:\n",
    "# for images, labels in train_loader:\n",
    "#     feats = feature_builder.extract_features(images[0])\n",
    "\n",
    "# ====================================================\n",
    "# c) Split dataset\n",
    "# ====================================================\n",
    "splitter = DatasetSplitter(DATASET_DIR, WORK_DIR, train_ratio=0.8)\n",
    "splitter.split()\n",
    "\n",
    "# ====================================================\n",
    "# 1️⃣ Load datasets with DataLoaders\n",
    "# ====================================================\n",
    "loader = LoadData(WORK_DIR, transform=transform)\n",
    "train_loader, val_loader, class_names = loader.load_and_split(batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================================================\n",
    "# d) Build and train the first model\n",
    "# ====================================================\n",
    "num_classes = len(class_names)\n",
    "model_builder = ModelBuilder(DEVICE, num_classes=num_classes)\n",
    "model = model_builder.build()\n",
    "\n",
    "trainer = Trainer(DEVICE)\n",
    "trainer.train(model, train_loader, val_loader, epochs=EPOCHS, lr=LR)\n",
    "\n",
    "# ====================================================\n",
    "# e) Hyperparameter tuning\n",
    "# ====================================================\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 1e-4],\n",
    "    \"epochs\": [3, 5]\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(param_grid, DEVICE)\n",
    "best_params, best_acc = tuner.tune(train_loader, val_loader, save_path=SAVE_PATH)\n",
    "print(f\"Best hyperparameters: {best_params}, Best validation accuracy: {best_acc:.3f}\")\n",
    "\n",
    "# ====================================================\n",
    "# f) Evaluate model\n",
    "# ====================================================\n",
    "evaluator = Evaluator(DEVICE)\n",
    "best_model = Predictor(DEVICE, class_names).load_model(SAVE_PATH)\n",
    "val_accuracy = evaluator.evaluate(best_model, val_loader)\n",
    "print(f\"Validation accuracy of the best model: {val_accuracy:.3f}\")\n",
    "\n",
    "# ====================================================\n",
    "# g) Predict new images (optional)\n",
    "# ====================================================\n",
    "# predictor = Predictor(DEVICE, class_names)\n",
    "# image_path = \"../dataset/test/alpaca_01.jpg\"\n",
    "# label, confidence = predictor.predict(image_path, best_model, transform)\n",
    "# print(f\"Predicted label: {label}, Confidence: {confidence:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project CFDS - Pipeline & Tuning\n",
    "\n",
    "This notebook demonstrates how to use the `best_library` to run the end-to-end machine learning pipeline and perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path to access src\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from best_library import LoadData, Preprocessing, FeatureBuilder, DatasetSplitter, ModelBuilder, Trainer, Predictor, HyperparameterTuner, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standard Pipeline Execution\n",
    "\n",
    "Here we define our configuration, split the data, compute statistics, and train a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Note: Paths are relative to the notebook location\n",
    "DATASET_DIR = \"../dataset\"\n",
    "WORK_DIR = \"../data\"\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1️⃣ Load data ---\n",
    "loader = LoadData(WORK_DIR, transform=None)  # transform encara no aplicat\n",
    "train_loader, val_loader, class_names = loader.load_and_split(batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split Data\n",
    "split_dataset(DATASET_DIR, WORK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Engineering (Compute Stats)\n",
    "train_dir = os.path.join(WORK_DIR, \"train\")\n",
    "if os.path.exists(train_dir):\n",
    "    compute_dataset_stats(train_dir, img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocessing & Data Loading\n",
    "preprocessing = Preprocessing(img_size=IMG_SIZE)\n",
    "transform = preprocessing.get_transform()\n",
    "\n",
    "train_loader, val_loader, class_names = load_data(WORK_DIR, BATCH_SIZE, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build & Train Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = build_model(device, num_classes=len(class_names))\n",
    "\n",
    "train_model(model, train_loader, val_loader, EPOCHS, LR, device, save_path=\"../alpaca_classifier_notebook.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning\n",
    "\n",
    "Now we use the `HyperparameterTuner` to find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'batch_size': [8, 16],\n",
    "    'epochs': [3] # Keeping it short for demo\n",
    "}\n",
    "\n",
    "print(\"Initializing Tuner...\")\n",
    "tuner = HyperparameterTuner(WORK_DIR, param_grid, img_size=IMG_SIZE)\n",
    "\n",
    "best_params, best_acc = tuner.tune()\n",
    "\n",
    "print(f\"Optimization finished! Best Params: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project-cfds (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
